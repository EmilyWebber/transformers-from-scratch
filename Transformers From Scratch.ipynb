{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers From Scratch\n",
    "You've heard the name, you've seen the papers, you've probably even stepped through a few repos. But can you build a multi-head attention mechanism from scratch?\n",
    "\n",
    "In this lab you'll be asked to implement the novel multi-head attention function from scratch, and plug this into our provided framework to test that your function works.\n",
    "\n",
    "We'll also ask you to parralellize it to take advantage of all the cores in your machine. \n",
    "\n",
    "So hang in there, and let's get to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 0 - Learn About This Script Pacakge\n",
    "First, let's examine the codebase here. You'll see a Transformer-based model architecture, so to speak, implemented here. The catch? There is no multi-head attention function. \n",
    "\n",
    "We'll step through the codebase with you here in the guide so you know the key points. At the end we'll run a quick functional test to make sure you can indeed run a basic training job.\n",
    "\n",
    "Task 0 should take ~ 10 minutes. Try to take your time reading through this so you know what the arguments are and how the script is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'former'...\n",
      "remote: Enumerating objects: 43, done.\u001b[K\n",
      "remote: Counting objects: 100% (43/43), done.\u001b[K\n",
      "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
      "remote: Total 138 (delta 19), reused 31 (delta 11), pack-reused 95\u001b[K\n",
      "Receiving objects: 100% (138/138), 34.90 MiB | 72.34 MiB/s, done.\n",
      "Resolving deltas: 100% (55/55), done.\n"
     ]
    }
   ],
   "source": [
    "# this package is a fork of Peter Bloem's \"former\" repository, where he implements a Transformer from scratch in PyTorch.\n",
    "!git clone https://github.com/EmilyWebber/former.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, this codebase supports two modes. One for text classification, the other for text generation. Both of those can be run by a single Python script, `python experiments/classify.py`. You'll see that we are just calling that Python script inside of the `model.py` script that SageMaker uses to execute the training job.\n",
    "\n",
    "Inside of `classify.py`, you'll see we're creating a model on line 60. `model = former.CTransformer`. Note that this model takes a few hyperparameters - `embedding_size`, `num_heads`, `depth`, `seq_length`, `num_classes`, and `max_pool`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/model_create.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check out that `CTransformer()` object. You'll see it's inherited from the `former` class. \n",
    "\n",
    "Inside `transformers.py`, there's an `__init__` for the `CTransformer()` object. Inside the init, we'll see a small for-loop defined that pulls in the hyperparameters we just passed in, and creates a `TransformerBlock`. Then, it's using the PyTorch `nn.Sequential` API to convert those blocks into a sequential neural network component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/TransformerBlock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, where is this `TransformerBlock` defined? It's actually in `modules.py`.  You'll see one `TransformerBlock` class, that inherits a `SelfAttentionWide` or `SelfAttentionNarrow` object on creation. We'll follow the rabbit-hole on the `SelfAttentionNarrow.`\n",
    "\n",
    "The `SelfAttentionWide` init defines a few objects: `self.tokeys`, `self.toqueries`, and `self.tovalues`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tokeys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we see a `forward` function that calls `self.tokeys`. Next, it computes a scaled dot-product self-attention function! That's what we need to implement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/self-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sound like fun? Now, run your base training job to make sure the pipes are working today. This should download the data onto the training host, but it won't actually train because we haven't implemented your solution yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# run the model file\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = 'transformers'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create some arbitrary train file that we won't use\n",
    "!echo 1,2,3,4 > holder_file.csv \n",
    "s3_train_path = \"s3://{}/{}/train/{}\".format(bucket, prefix, 'holder_file.csv')\n",
    "os.system('aws s3 cp {} {}'.format( 'holder_file.csv', s3_train_path))\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='model.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.2.0',\n",
    "                    py_version = 'py3',\n",
    "                    source_dir = 'former',\n",
    "                    train_instance_count=1,\n",
    "                    train_instance_type = 'ml.p3.2xlarge')\n",
    "\n",
    "estimator.fit({'training': s3_train_path}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 1 - Implement a Multi-head Attention Function\n",
    "Now, here's the fun part. Can you implement your own multi-head attention mechanism? Don't worry, we'll give you all the tips you need.\n",
    "\n",
    "Task 1 should take ~30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting absl-py==0.7.1\n",
      "  Downloading absl-py-0.7.1.tar.gz (99 kB)\n",
      "\u001b[K     |████████████████████████████████| 99 kB 2.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: chardet==3.0.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from -r former/requirements.txt (line 3)) (3.0.4)\n",
      "Collecting future==0.17.1\n",
      "  Downloading future-0.17.1.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 9.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio==1.23.0\n",
      "  Downloading grpcio-1.23.0-cp36-cp36m-manylinux1_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 15.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna==2.8\n",
      "  Downloading idna-2.8-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 8.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting markdown==3.1.1\n",
      "  Downloading Markdown-3.1.1-py2.py3-none-any.whl (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 9.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting numpy==1.17.0\n",
      "  Downloading numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 20.4 MB 21.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf==3.9.1\n",
      "  Downloading protobuf-3.9.1-cp36-cp36m-manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 51.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests==2.22.0\n",
      "  Downloading requests-2.22.0-py2.py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 9.4 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting six==1.12.0\n",
      "  Downloading six-1.12.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tb-nightly==1.15.0a20190822\n",
      "  Downloading tb_nightly-1.15.0a20190822-py3-none-any.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 44.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torch==1.2.0\n",
      "  Downloading torch-1.2.0-cp36-cp36m-manylinux1_x86_64.whl (748.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8 MB 8.0 kB/s  eta 0:00:01    |██▉                             | 66.0 MB 61.3 MB/s eta 0:00:12     |████████▍                       | 195.2 MB 80.8 MB/s eta 0:00:07     |███████████▍                    | 265.5 MB 54.5 MB/s eta 0:00:09     |███████████▋                    | 271.9 MB 54.5 MB/s eta 0:00:09     |█████████████▍                  | 312.2 MB 54.5 MB/s eta 0:00:09     |█████████████████████▋          | 506.5 MB 65.1 MB/s eta 0:00:04\n",
      "\u001b[?25hCollecting torchtext==0.4.0\n",
      "  Downloading torchtext-0.4.0-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 3.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.34.0\n",
      "  Downloading tqdm-4.34.0-py2.py3-none-any.whl (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3==1.25.3\n",
      "  Downloading urllib3-1.25.3-py2.py3-none-any.whl (150 kB)\n",
      "\u001b[K     |████████████████████████████████| 150 kB 74.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug==0.15.5\n",
      "  Downloading Werkzeug-0.15.5-py2.py3-none-any.whl (328 kB)\n",
      "\u001b[K     |████████████████████████████████| 328 kB 61.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=36 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from markdown==3.1.1->-r former/requirements.txt (line 7)) (46.1.3.post20200330)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests==2.22.0->-r former/requirements.txt (line 10)) (2020.6.20)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tb-nightly==1.15.0a20190822->-r former/requirements.txt (line 12)) (0.34.2)\n",
      "Building wheels for collected packages: absl-py, future\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.7.1-py3-none-any.whl size=117847 sha256=4926bfcef853fdbe192dc073981454d5de81a793f7d72d83542686475f22604a\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/e6/18/55/72da70503ed709c86faee3a9d3034befffc0f6d76d22e57104\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.17.1-py3-none-any.whl size=488732 sha256=e4af76f00f85d9ee07d8b365e4b7e94d3b205ce3ab1b0fa10b5f26ff3f65e905\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/c4/f0/e2/8e4ecc9e1b12a428a7657ba683576d3e79d0a75982f63e8fd2\n",
      "Successfully built absl-py future\n",
      "\u001b[31mERROR: pytest-astropy 0.8.0 requires pytest-cov>=2.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: pytest-astropy 0.8.0 requires pytest-filter-subpackage>=0.1, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: jupyterlab 1.2.6 has requirement jupyterlab_server~=1.0.0, but you'll have jupyterlab-server 1.1.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: six, absl-py, future, grpcio, idna, markdown, numpy, protobuf, urllib3, requests, werkzeug, tb-nightly, torch, tqdm, torchtext\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.14.0\n",
      "    Uninstalling six-1.14.0:\n",
      "      Successfully uninstalled six-1.14.0\n",
      "  Attempting uninstall: future\n",
      "    Found existing installation: future 0.18.2\n",
      "    Uninstalling future-0.18.2:\n",
      "      Successfully uninstalled future-0.18.2\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 2.9\n",
      "    Uninstalling idna-2.9:\n",
      "      Successfully uninstalled idna-2.9\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.1\n",
      "    Uninstalling numpy-1.18.1:\n",
      "      Successfully uninstalled numpy-1.18.1\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.13.0\n",
      "    Uninstalling protobuf-3.13.0:\n",
      "      Successfully uninstalled protobuf-3.13.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.25.8\n",
      "    Uninstalling urllib3-1.25.8:\n",
      "      Successfully uninstalled urllib3-1.25.8\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.23.0\n",
      "    Uninstalling requests-2.23.0:\n",
      "      Successfully uninstalled requests-2.23.0\n",
      "  Attempting uninstall: werkzeug\n",
      "    Found existing installation: Werkzeug 1.0.1\n",
      "    Uninstalling Werkzeug-1.0.1:\n",
      "      Successfully uninstalled Werkzeug-1.0.1\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.44.1\n",
      "    Uninstalling tqdm-4.44.1:\n",
      "      Successfully uninstalled tqdm-4.44.1\n",
      "Successfully installed absl-py-0.7.1 future-0.17.1 grpcio-1.23.0 idna-2.8 markdown-3.1.1 numpy-1.17.0 protobuf-3.11.4 requests-2.22.0 six-1.12.0 tb-nightly-1.15.0a20190822 torch-1.2.0 torchtext-0.4.0 tqdm-4.34.0 urllib3-1.25.3 werkzeug-0.15.5\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r former/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Used for converting between nats and bits\n",
    "LOG2E = math.log2(math.e)\n",
    "TEXT = data.Field(lower=True, include_lengths=True, batch_first=True)\n",
    "LABEL = data.Field(sequential=False)\n",
    "NUM_CLS = 2\n",
    "\n",
    "def d(tensor=None):\n",
    "    if tensor is None:\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    return 'cuda' if tensor.is_cuda else 'cpu'\n",
    "\n",
    "def get_x():\n",
    "    \n",
    "    tdata, _ = datasets.IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "    print ('calling imbdb.splits')\n",
    "\n",
    "    train, test = tdata.split(split_ratio=0.8)\n",
    "\n",
    "    print ('loading train and test sets')\n",
    "    TEXT.build_vocab(train, max_size=50_000 - 2) # - 2 to make space for <unk> and <pad>\n",
    "    LABEL.build_vocab(train)\n",
    "\n",
    "    train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size=4, device=d())\n",
    "\n",
    "    return train_iter, test_iter\n",
    "\n",
    "# it will take a few minutes to run this cell, even on an m5.large\n",
    "# go ahead and run this cell. while you're waiting for it to complete, jump ahead to the next portion\n",
    "# tbw = SummaryWriter(log_dir='./runs') # Tensorboard logging\n",
    "# train_iter, _ = get_x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is where you define your first three layers, for the keys, queries, and values\n",
    "def get_inputs(emb = 512, heads=8):\n",
    "    \n",
    "    tokeys = # your code here \n",
    "    toqueries = # your code here \n",
    "    tovalues = # your code here \n",
    "    \n",
    "    keys    = # your code here \n",
    "    queries = # your code here \n",
    "    values  = # your code here \n",
    "    \n",
    "    return keys, queries, values\n",
    "\n",
    "def my_multihead_attention_mechanism(keys, queries, values):\n",
    "        # compute scaled dot-product self-attention\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = # your code here \n",
    "        queries = # your code here \n",
    "        values = # your code here \n",
    "\n",
    "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "        #   This should be more memory efficient\n",
    "        \n",
    "        queries = # your code here \n",
    "        keys    = # your code here \n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = # your code here \n",
    "\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # - dot now has row-wise self-attention probabilities\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = # your code here \n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = # your code here \n",
    "    \n",
    "        self.unifyheads = # your code here \n",
    "\n",
    "        return self.unifyheads(out)\n",
    "    \n",
    "keys, queries, values = get_inputs()\n",
    "    \n",
    "my_multihead_attention_mechanism(keys, queries, values)\n",
    "\n",
    "# then we apply the train iter object to the compiled neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THE ONE TO USE FOR TESTING \n",
    "\n",
    "# this is where you define your first three layers, for the keys, queries, and values\n",
    "def get_inputs(emb = 512, heads=8):\n",
    "    \n",
    "    tokeys = nn.Linear(emb, emb * heads, bias=False)\n",
    "    toqueries = nn.Linear(emb, emb * heads, bias=False)\n",
    "    tovalues = nn.Linear(emb, emb * heads, bias=False)\n",
    "    \n",
    "    keys    = tokeys(x)\n",
    "    queries = toqueries(x)\n",
    "    values  = tovalues(x)\n",
    "    \n",
    "    return keys, queries, values\n",
    "\n",
    "def my_multihead_attention_mechanism(keys, queries, values):\n",
    "        # compute scaled dot-product self-attention\n",
    "\n",
    "        # - fold heads into the batch dimension\n",
    "        keys = keys.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        queries = queries.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "        values = values.transpose(1, 2).contiguous().view(b * h, t, e)\n",
    "\n",
    "        queries = queries / (e ** (1/4))\n",
    "        keys    = keys / (e ** (1/4))\n",
    "        # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "        #   This should be more memory efficient\n",
    "\n",
    "        # - get dot product of queries and keys, and scale\n",
    "        dot = torch.bmm(queries, keys.transpose(1, 2))\n",
    "\n",
    "        assert dot.size() == (b*h, t, t)\n",
    "\n",
    "        if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "            mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "        dot = F.softmax(dot, dim=2)\n",
    "        # - dot now has row-wise self-attention probabilities\n",
    "\n",
    "        # apply the self attention to the values\n",
    "        out = torch.bmm(dot, values).view(b, h, t, e)\n",
    "\n",
    "        # swap h, t back, unify heads\n",
    "        out = out.transpose(1, 2).contiguous().view(b, t, h * e)\n",
    "    \n",
    "        self.unifyheads = nn.Linear(heads * emb, emb)\n",
    "\n",
    "        return self.unifyheads(out)\n",
    "    \n",
    "keys, queries, values = get_inputs()\n",
    "    \n",
    "my_multihead_attention_mechanism(keys, queries, values)\n",
    "\n",
    "# then we apply the train iter object to the compiled neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great stuff! Now, let's get that incorporated into the entire script we defined above. Paste your function into the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_transformer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_transformer_model.py\n",
    "\n",
    "# << paste your function here >> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test that out on SageMaker! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your new transformer script on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's a wrap! If you made it here with extra time to spare, consider doubling back on some of those functions and try to really understand what's going on there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
