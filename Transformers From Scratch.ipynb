{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers From Scratch\n",
    "You've heard the name, you've seen the papers, you've probably even stepped through a few repos. But can you build a multi-head attention mechanism from scratch?\n",
    "\n",
    "In this lab you'll be asked to implement the novel multi-head attention function from scratch, and plug this into our provided framework to test that your function works.\n",
    "\n",
    "We'll also ask you to parralellize it to take advantage of all the cores in your machine. \n",
    "\n",
    "So hang in there, and let's get to it!\n",
    "\n",
    "* Note the computational and data needs of this lab are pretty intensive. I was able to run successfully on a notebook instance using ml.p3.16xlarge. If you don't already have that instance you might want to cut a ticket for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 0 - Learn About This Script Pacakge\n",
    "First, let's examine the codebase here. You'll see a Transformer-based model architecture, so to speak, implemented here. The catch? There is no multi-head attention function. \n",
    "\n",
    "We'll step through the codebase with you here in the guide so you know the key points. At the end we'll run a quick functional test to make sure you can indeed run a basic training job.\n",
    "\n",
    "Task 0 should take ~ 10 minutes. Try to take your time reading through this so you know what the arguments are and how the script is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this package is a fork of Peter Bloem's \"former\" repository, where he implements a Transformer from scratch in PyTorch.\n",
    "!git clone https://github.com/EmilyWebber/former.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, this codebase supports two modes. One for text classification, the other for text generation. Both of those can be run by a single Python script, `python experiments/classify.py`. You'll see that we are just calling that Python script inside of the `model.py` script that SageMaker uses to execute the training job.\n",
    "\n",
    "Inside of `classify.py`, you'll see we're creating a model on line 60. `model = former.CTransformer`. Note that this model takes a few hyperparameters - `embedding_size`, `num_heads`, `depth`, `seq_length`, `num_classes`, and `max_pool`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/model_create.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check out that `CTransformer()` object. You'll see it's inherited from the `former` class. \n",
    "\n",
    "Inside `transformers.py`, there's an `__init__` for the `CTransformer()` object. Inside the init, we'll see a small for-loop defined that pulls in the hyperparameters we just passed in, and creates a `TransformerBlock`. Then, it's using the PyTorch `nn.Sequential` API to convert those blocks into a sequential neural network component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/TransformerBlock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, where is this `TransformerBlock` defined? It's actually in `modules.py`.  You'll see one `TransformerBlock` class, that inherits a `SelfAttentionWide` or `SelfAttentionNarrow` object on creation. We'll follow the rabbit-hole on the `SelfAttentionNarrow.`\n",
    "\n",
    "The `SelfAttentionWide` init defines a few objects: `self.tokeys`, `self.toqueries`, and `self.tovalues`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tokeys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we see a `forward` function that calls `self.tokeys`. Next, it computes a scaled dot-product self-attention function! That's what we need to implement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/self-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sound like fun? Now, run your base training job to make sure the pipes are working today. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model file\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = 'transformers'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create some arbitrary train file that we won't use\n",
    "!echo 1,2,3,4 > holder_file.csv \n",
    "s3_train_path = \"s3://{}/{}/train/{}\".format(bucket, prefix, 'holder_file.csv')\n",
    "os.system('aws s3 cp {} {}'.format( 'holder_file.csv', s3_train_path))\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='model.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.2.0',\n",
    "                    py_version = 'py3',\n",
    "                    source_dir = 'former',\n",
    "                    instance_count=1,\n",
    "                    instance_type = 'ml.p3.2xlarge')\n",
    "\n",
    "estimator.fit({'training': s3_train_path}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 1 - Implement a Multi-head Attention Function\n",
    "Now, here's the fun part. Can you implement your own multi-head attention mechanism? Don't worry, we'll give you all the tips you need.\n",
    "\n",
    "Task 1 should take ~30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install -r former/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you're stepping through the code below to build your neural network, don't panic. You can actually look at the pictures provided in this notebook here, along with the code from the `former` repo, for inspiration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting former/former/my_transformer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile former/former/my_transformer_model.py\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torchtext import data, datasets, vocab\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def my_multihead_attention_mechanism(x, emb = 128, h = 8):\n",
    "    \n",
    "    b, t, e = # your code here\n",
    "    assert e == emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
    "\n",
    "    s = # your code here \n",
    "    x = # your code here \n",
    "\n",
    "    tokeys = # your code here \n",
    "    toqueries = # your code here \n",
    "    tovalues  = # your code here \n",
    "\n",
    "    keys = # your code here \n",
    "    queries = # your code here \n",
    "    values  = # your code here \n",
    "    \n",
    "    assert keys.size() == (b, t, h, s)\n",
    "    assert queries.size() == (b, t, h, s)\n",
    "    assert values.size() == (b, t, h, s)\n",
    "    \n",
    "    # compute scaled dot-product self-attention\n",
    "\n",
    "    # - fold heads into the batch dimension\n",
    "    keys = # your code here \n",
    "    queries = # your code here \n",
    "    values = # your code here \n",
    "\n",
    "    queries = # your code here \n",
    "    keys    = # your code here \n",
    "    # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
    "    #   This should be more memory efficient\n",
    "\n",
    "    # - get dot product of queries and keys, and scale\n",
    "    dot = # your code here \n",
    "\n",
    "    assert dot.size() == (b*h, t, t)\n",
    "\n",
    "    if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
    "        mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
    "\n",
    "    dot = # your code here \n",
    "    # - dot now has row-wise self-attention probabilities\n",
    "\n",
    "    # apply the self attention to the values\n",
    "    out = # your code here \n",
    "\n",
    "    # swap h, t back, unify heads\n",
    "    out = # your code here \n",
    "\n",
    "    unifyheads = # your code here \n",
    "\n",
    "    return unifyheads(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out this package is just happier being run as a script - when you run this line it's pointing to a string of custom objects that will ultimately look for your `my_attention_mechanism` function as the base of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIONS  Namespace(batch_size=4, depth=6, embedding_size=128, final=False, gradient_clipping=1.0, lr=0.0001, lr_warmup=10000, max_length=512, max_pool=False, num_epochs=80, num_heads=8, seed=1, tb_dir='./runs', vocab_size=50000)\n",
      "- nr. of training examples 5000\n",
      "- nr. of validation examples 1250\n",
      "\n",
      " epoch 0\n",
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]looping through batches\n",
      "got our first input!\n",
      "  0%|                                                  | 0/5000 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!python former/experiments/custom_classify.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may be thinking, hey, I thought we were going to train a full model here? And the sad reality is that, at 11pm the night before the workshop, I simply ran out of steam to continue debugging the error statements on the custom classifier. If you are interested in and willing it give it a shot, please by all means do so. Otherwise, on we go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test that out on SageMaker! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting former/my_new_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile former/my_new_model.py\n",
    "\n",
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    os.system('python experiments/custom_classify.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model file\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "estimator = PyTorch(entry_point='my_new_model.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.2.0',\n",
    "                    py_version = 'py3',\n",
    "                    source_dir = 'former',\n",
    "                    instance_count=1,\n",
    "                    instance_type = 'ml.p3.2xlarge')\n",
    "\n",
    "estimator.fit({'training': s3_train_path}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 2 - Compare your Results\n",
    "When we have about 30 minutes left, the MC team will share the solution notebook with you so you can compare notes. Should take only a few minutes if your solution is working, otherwise you may want to step through it a bit more carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's a wrap! If you made it here with extra time to spare, consider doubling back on some of those functions and try to really understand what's going on there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's a wrap! If you made it here with extra time to spare, consider doubling back on some of those functions and try to really understand what's going on there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
