{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers From Scratch\n",
    "You've heard the name, you've seen the papers, you've probably even stepped through a few repos. But can you build a multi-head attention mechanism from scratch?\n",
    "\n",
    "In this lab you'll be asked to implement the novel multi-head attention function from scratch, and plug this into our provided framework to test that your function works.\n",
    "\n",
    "We'll also ask you to parralellize it to take advantage of all the cores in your machine. \n",
    "\n",
    "So hang in there, and let's get to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 0 - Learn About This Script Pacakge\n",
    "First, let's examine the codebase here. You'll see a Transformer-based model architecture, so to speak, implemented here. The catch? There is no multi-head attention function. \n",
    "\n",
    "We'll step through the codebase with you here in the guide so you know the key points. At the end we'll run a quick functional test to make sure you can indeed run a basic training job.\n",
    "\n",
    "Task 0 should take ~ 10 minutes. Try to take your time reading through this so you know what the arguments are and how the script is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'former' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# this package is a fork of Peter Bloem's \"former\" repository, where he implements a Transformer from scratch in PyTorch.\n",
    "!git clone https://github.com/EmilyWebber/former.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, this codebase supports two modes. One for text classification, the other for text generation. Both of those can be run by a single Python script, `python experiments/classify.py`. You'll see that we are just calling that Python script inside of the `model.py` script that SageMaker uses to execute the training job.\n",
    "\n",
    "Inside of `classify.py`, you'll see we're creating a model on line 60. `model = former.CTransformer`. Note that this model takes a few hyperparameters - `embedding_size`, `num_heads`, `depth`, `seq_length`, `num_classes`, and `max_pool`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/model_create.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check out that `CTransformer()` object. You'll see it's inherited from the `former` class. \n",
    "\n",
    "Inside `transformers.py`, there's an `__init__` for the `CTransformer()` object. Inside the init, we'll see a small for-loop defined that pulls in the hyperparameters we just passed in, and creates a `TransformerBlock`. Then, it's using the PyTorch `nn.Sequential` API to convert those blocks into a sequential neural network component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/TransformerBlock.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, where is this `TransformerBlock` defined? It's actually in `modules.py`.  You'll see one `TransformerBlock` class, that inherits a `SelfAttentionWide` or `SelfAttentionNarrow` object on creation. We'll follow the rabbit-hole on the `SelfAttentionWide.`\n",
    "\n",
    "The `SelfAttentionWide` init defines a few objects: `self.tokeys`, `self.toqueries`, and `self.tovalues`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tokeys.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we see a `forward` function that calls `self.tokeys`. Next, it computes a scaled dot-product self-attention function! That's what we need to implement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/self-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sound like fun? Now, run your base training job to make sure the pipes are working today. This should download the data onto the training host, but it won't actually train because we haven't implemented your solution yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the model file\n",
    "import sagemaker\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "prefix = 'transformers'\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# create some arbitrary train file that we won't use\n",
    "!echo 1,2,3,4 > holder_file.csv \n",
    "s3_train_path = \"s3://{}/{}/train/{}\".format(bucket, prefix, 'holder_file.csv')\n",
    "os.system('aws s3 cp {} {}'.format( 'holder_file.csv', s3_train_path))\n",
    "\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='model.py',\n",
    "                    role=role,\n",
    "                    framework_version='1.2.0',\n",
    "                    py_version = 'py3',\n",
    "                    source_dir = 'former',\n",
    "                    instance_count=1,\n",
    "                    # if you haven't already cut a ticket for this instance type, go ahead and do it. this will dramatically improve your training time! \n",
    "                    instance_type = 'ml.p3dn.24xlarge')\n",
    "\n",
    "estimator.fit({'training': s3_train_path}, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 1 - Implement a Multi-head Attention Function\n",
    "Now, here's the fun part. Can you implement your own multi-head attention mechanism? Don't worry, we'll give you all the tips you need.\n",
    "\n",
    "Task 1 should take ~30 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let's do this!!\n"
     ]
    }
   ],
   "source": [
    "def my_multihead_attention_mechanism():\n",
    "    print (\"let's do this!!\")\n",
    "    \n",
    "my_multihead_attention_mechanism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great stuff! Now, let's get that incorporated into the entire script we defined above. Paste your function into the script below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_transformer_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_transformer_model.py\n",
    "\n",
    "# << paste your function here >> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test that out on SageMaker! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run your new transformer script on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 2 - Parallelize your Multihead Attention Function\n",
    "That seemed surprisingly doable, right? Now remember that a major advantage of transformers is their ability to take advantage of multiple GPU's, or compute cores, in a single host. Let's see if we can implement that here!\n",
    "\n",
    "Task 2 should take ~ 30 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's import Python's `multiprocessing` package. Also grab the `Pool` object from within `multiprocessing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the multiprocessing package\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, try to call the method from multiprocessing, `.cpu_count()`, to confirm how many cores you are running on right now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# num_cpus = # your function here\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "print (num_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set up a `Pool()` object that takes the number of cpus, defined above, as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = # your code here\n",
    "pool = Pool(num_cpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `multiprocessing` package makes this super easy for us. All we need to do is define a `data_to_map` variable that's literally holding all of the data we want to map out to each core, plus an `attention_function` that will be called on each of those matrices. Let's give it a shot! As a hint, you may want to simply consider using a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_map = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use your attention function definition from above, now formatted to work with each object in your `data_to_map` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_function():\n",
    "    print ('all the attention')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we just want to pass the `attention_function` and the `data_to_map` objects into the `pool` variable you created above. As a hint, you might consider something like this:   `new_rows = pool.map(function_name, data_obj)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_rows = pool.map(attention_function, data_to_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `concat` at the end of the attention mechanisms? Let's implement that right here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task 3 - Compare against solution set\n",
    "If you made it here with tons of time to spare, congrats! We will release the solution notebook 70 minutes into the lab, so that you definitely have a shot at implementing the attention mechanism yourself.\n",
    "\n",
    "This last task is optional - just so you can compare your solutions against ours and make sure we're all on the right track.\n",
    "\n",
    "Task 3 should take ~ 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: transformers-from-scratch/.ipynb_checkpoints/Transformers From Scratch-checkpoint.ipynb to s3://notebook-etc/transformers-from-scratch/.ipynb_checkpoints/Transformers From Scratch-checkpoint.ipynb\n",
      "upload: transformers-from-scratch/Transformers From Scratch.ipynb to s3://notebook-etc/transformers-from-scratch/Transformers From Scratch.ipynb\n",
      "upload: transformers-from-scratch/former/.git/HEAD to s3://notebook-etc/transformers-from-scratch/former/.git/HEAD\n",
      "upload: transformers-from-scratch/former/.git/config to s3://notebook-etc/transformers-from-scratch/former/.git/config\n",
      "upload: transformers-from-scratch/former/.git/description to s3://notebook-etc/transformers-from-scratch/former/.git/description\n",
      "upload: transformers-from-scratch/former/.git/hooks/applypatch-msg.sample to s3://notebook-etc/transformers-from-scratch/former/.git/hooks/applypatch-msg.sample\n",
      "upload: transformers-from-scratch/former/.git/hooks/post-update.sample to s3://notebook-etc/transformers-from-scratch/former/.git/hooks/post-update.sample\n",
      "upload: transformers-from-scratch/former/.git/hooks/pre-applypatch.sample to s3://notebook-etc/transformers-from-scratch/former/.git/hooks/pre-applypatch.sample\n",
      "upload: transformers-from-scratch/former/.git/hooks/commit-msg.sample to s3://notebook-etc/transformers-from-scratch/former/.git/hooks/commit-msg.sample\n",
      "upload: transformers-from-scratch/former/.git/hooks/fsmonitor-watchman.sample to s3://notebook-etc/transformers-from-scratch/former/.git/hooks/fsmonitor-watchman.sample\n",
      "upload: transformers-from-scratch/former/.git/logs/HEAD to s3://notebook-etc/transformers-from-scratch/former/.git/logs/HEAD\n",
      "upload: transformers-from-scratch/former/.git/hooks/update.sample to s3://notebook-etc/transformers-from-scratch/former/.git/hooks/update.sample\n",
      "upload: transformers-from-scratch/former/.git/hooks/pre-commit.sample to s3://notebook-etc/transformers-from-scratch/former/.git/hooks/pre-commit.sample\n",
      "upload: transformers-from-scratch/former/.git/index to s3://notebook-etc/transformers-from-scratch/former/.git/index\n",
      "upload: transformers-from-scratch/former/.git/logs/refs/heads/master to s3://notebook-etc/transformers-from-scratch/former/.git/logs/refs/heads/master\n",
      "upload: transformers-from-scratch/former/.git/logs/refs/remotes/origin/HEAD to s3://notebook-etc/transformers-from-scratch/former/.git/logs/refs/remotes/origin/HEAD\n",
      "upload: transformers-from-scratch/former/.git/objects/pack/pack-7be9d5304b1aaf4732cb90fdb2b47d26e45b1265.idx to s3://notebook-etc/transformers-from-scratch/former/.git/objects/pack/pack-7be9d5304b1aaf4732cb90fdb2b47d26e45b1265.idx\n",
      "upload: transformers-from-scratch/former/.git/hooks/prepare-commit-msg.sample to s3://notebook-etc/transformers-from-scratch/former/.git/hooks/prepare-commit-msg.sample\n",
      "upload: transformers-from-scratch/former/.git/hooks/pre-rebase.sample to s3://notebook-etc/transformers-from-scratch/former/.git/hooks/pre-rebase.sample\n",
      "upload: transformers-from-scratch/former/.git/hooks/pre-receive.sample to s3://notebook-etc/transformers-from-scratch/former/.git/hooks/pre-receive.sample\n",
      "upload: transformers-from-scratch/former/.git/hooks/pre-push.sample to s3://notebook-etc/transformers-from-scratch/former/.git/hooks/pre-push.sample\n",
      "upload: transformers-from-scratch/former/.git/info/exclude to s3://notebook-etc/transformers-from-scratch/former/.git/info/exclude\n",
      "upload: transformers-from-scratch/former/.git/packed-refs to s3://notebook-etc/transformers-from-scratch/former/.git/packed-refs\n",
      "upload: transformers-from-scratch/former/.git/refs/heads/master to s3://notebook-etc/transformers-from-scratch/former/.git/refs/heads/master\n",
      "upload: transformers-from-scratch/former/.git/refs/remotes/origin/HEAD to s3://notebook-etc/transformers-from-scratch/former/.git/refs/remotes/origin/HEAD\n",
      "upload: transformers-from-scratch/former/LICENSE to s3://notebook-etc/transformers-from-scratch/former/LICENSE\n",
      "upload: transformers-from-scratch/former/README.md to s3://notebook-etc/transformers-from-scratch/former/README.md\n",
      "upload: transformers-from-scratch/former/data/README.md to s3://notebook-etc/transformers-from-scratch/former/data/README.md\n",
      "upload: transformers-from-scratch/former/environment.yml to s3://notebook-etc/transformers-from-scratch/former/environment.yml\n",
      "upload: transformers-from-scratch/former/experiments/classify.py to s3://notebook-etc/transformers-from-scratch/former/experiments/classify.py\n",
      "upload: transformers-from-scratch/former/experiments/_context.py to s3://notebook-etc/transformers-from-scratch/former/experiments/_context.py\n",
      "upload: transformers-from-scratch/former/experiments/generate.py to s3://notebook-etc/transformers-from-scratch/former/experiments/generate.py\n",
      "upload: transformers-from-scratch/former/former/__init__.py to s3://notebook-etc/transformers-from-scratch/former/former/__init__.py\n",
      "upload: transformers-from-scratch/former/former/util/__init__.py to s3://notebook-etc/transformers-from-scratch/former/former/util/__init__.py\n",
      "upload: transformers-from-scratch/former/model.py to s3://notebook-etc/transformers-from-scratch/former/model.py\n",
      "upload: transformers-from-scratch/former/former/modules.py to s3://notebook-etc/transformers-from-scratch/former/former/modules.py\n",
      "upload: transformers-from-scratch/former/former/transformers.py to s3://notebook-etc/transformers-from-scratch/former/former/transformers.py\n",
      "upload: transformers-from-scratch/former/requirements.txt to s3://notebook-etc/transformers-from-scratch/former/requirements.txt\n",
      "upload: transformers-from-scratch/former/.git/objects/pack/pack-7be9d5304b1aaf4732cb90fdb2b47d26e45b1265.pack to s3://notebook-etc/transformers-from-scratch/former/.git/objects/pack/pack-7be9d5304b1aaf4732cb90fdb2b47d26e45b1265.pack\n",
      "upload: transformers-from-scratch/former/tests/_context.py to s3://notebook-etc/transformers-from-scratch/former/tests/_context.py\n",
      "upload: transformers-from-scratch/former/setup.py to s3://notebook-etc/transformers-from-scratch/former/setup.py\n",
      "upload: transformers-from-scratch/holder_file.csv to s3://notebook-etc/transformers-from-scratch/holder_file.csv\n",
      "upload: transformers-from-scratch/former/former/util/util.py to s3://notebook-etc/transformers-from-scratch/former/former/util/util.py\n",
      "upload: transformers-from-scratch/former/data/enwik8.gz to s3://notebook-etc/transformers-from-scratch/former/data/enwik8.gz\n"
     ]
    }
   ],
   "source": [
    "!cd ../ && aws s3 sync transformers-from-scratch s3://notebook-etc/transformers-from-scratch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
